{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8ID3hz8Hrw04Yy0V5AGIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faithselby/habermasagent/blob/main/HabermasAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iFQzeA0NOtQ0",
        "outputId": "bf383ded-fb42-471d-916d-5ae473af7c76"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.10.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Using cached chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Using cached posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Using cached onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Using cached opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\n",
            "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb)\n",
            "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=e8261ca7ac12c128fba39cd112ffcf363b791d67f2c086c03f70b64f9e558915\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, coloredlogs, build, tokenizers, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.2 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.41.3 tokenizers-0.20.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-lowKKc2tGPG",
        "outputId": "eaadc5c1-bccc-4942-ef48-9804daa0f699"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mistral_inference in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: fire>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: mistral_common>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (1.5.1)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (10.4.0)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.4.5)\n",
            "Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.1.6)\n",
            "Requirement already satisfied: xformers>=0.0.24 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.0.28.post3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.6.0->mistral_inference) (2.5.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (2.10.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (0.2.0)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (4.12.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing>=0.1.5->mistral_inference) (0.16)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from xformers>=0.0.24->mistral_inference) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->xformers>=0.0.24->mistral_inference) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.22.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->mistral_common>=1.4.0->mistral_inference) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch numpy huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UsCTDczct4n9",
        "outputId": "4a9837f2-4790-4341-c455-97d731fb1c15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PIPELINE"
      ],
      "metadata": {
        "id": "-cp9_Y9O2WeY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lU8UU00SCGzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CORE KNOWLEDGE BASE"
      ],
      "metadata": {
        "id": "77NyOPf7CEQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "habermas_analysis = {\n",
        "\n",
        "    \"Anti-Positivism\":{\n",
        "         \"description\": [\n",
        "              \"Characteristic of Habermas's work in the 1960s was its anti-positivism. \" +\n",
        "            \"In particular, he rejected the positivism of Marx's later writings and sought to turn the early work into a more effective springboard of an immanent critique of capitalist society by emphasising Marx's hermeneutic aspect.\",\n",
        "\n",
        "            \"This critique had the following features. First of all, Habermas argued that science, and even aspects of philosophy, had ceased to have a critical role in determining the worth of the ends to be pursued, and had instead become the slave of 'instrumental', or 'purposive' rationality (Weber's zweckrationalitat).\",\n",
        "\n",
        "            \"Science thus contributed to the technical rationality which enabled capitalism to develop more diverse and complex commodity forms, as well as sophisticated weaponry; it was, however, incapable of producing a creditable justification of the capitalist system itself. In short, the technical understanding of science was positivistic, and therefore ultimately ideological.\",\n",
        "\n",
        "            \"For it denied the hermeneutic component in science as it was practised. As a result, Habermas saw science and rationality in the capitalist era being turned against human beings – impoverishing their cultural lives, and exacerbating pathological forms – instead of being used for them.\",\n",
        "\n",
        "            \"Critical theory was needed to combat this negative form of positivistic science and turn it into an emancipatory activity concerned with political and social reform.\",\n",
        "\n",
        "            \"In contrast to Adorno and Horkheimer's pessimistic account of reason in the Dialectic of Enlightenment, Habermas seeks to turn the tide against such a negative conception and works to 'complete the project of modernity' begun in the Enlightenment.\",\n",
        "\n",
        "            \"Again, this goal necessitates a critique of the purely instrumentalist view of science dominant in post-war capitalism.\"\n",
        "        ],\n",
        "         \"key_concepts\": [\n",
        "\n",
        "            \"rejection of positivism\",\n",
        "            \"critique of instrumental rationality\",\n",
        "            \"hermeneutic approach\",\n",
        "            \"emancipatory science\",\n",
        "            \"completion of modernity project\"\n",
        "        ],\n",
        "    },\n",
        "\n",
        "  \"The State and Critique\":{\n",
        "      \"description\": [\n",
        "          \"Habermas's early work aimed to show how the modern state was an outcome of, and contributed to, capitalism's very survival.\",\n",
        "\n",
        "            \"In the 1970s, Habermas argued – drawing from political economists – that the state would not be able to cushion people from the worst excesses of the crises in the capitalist economy because its capacity to collect the revenue necessary to support welfare programmes was limited.\",\n",
        "\n",
        "            \"This limitation of state capacity, according to Habermas, entailed a limit to the state's legitimacy. The more the state became incapable of protecting people from economic crises, the less its legitimacy could be guaranteed.\",\n",
        "\n",
        "            \"Following the German idealist tradition, Habermas uses Marx to develop a strategy of critique which would be, as he sees it, essentially emancipatory.\",\n",
        "\n",
        "            \"While Marx emphasized the self-formative role of practical labour, Habermas, influenced by Hegel, sees labour as critique – particularly aimed against the numbing force of instrumental reason.\",\n",
        "\n",
        "            \"By demonstrating the practical achievements of the German hermeneutic tradition – in which Habermas includes Freud – he opens the way for a much greater emphasis on symbolic forms of interaction than Marx had ever envisaged.\"\n",
        "        ],\n",
        "      \"key_concepts\": [\n",
        "          \"state legitimacy crisis\",\n",
        "            \"welfare state limitations\",\n",
        "            \"critique as emancipation\",\n",
        "            \"labour as critique\",\n",
        "            \"symbolic interaction\",\n",
        "            \"hermeneutic tradition\"\n",
        "        ],\n",
        "  },\n",
        "\n",
        "    \"Theory of Communication\": {\n",
        "\n",
        "            \"description\": [\n",
        "                \"In the early 1970s, Habermas began formulating the initial elements of a theory of language, communication and societal evolution, aiming to establish a normative framework for realizing emancipatory interests.\",\n",
        "\n",
        "                \"This theoretical development culminated in his landmark work 'The Theory of Communicative Action', first published in Germany in 1981.\",\n",
        "\n",
        "                \"While Habermas maintains Marx's drive for emancipation, he rejects both revolutionary and positivistic methods of achieving it.\",\n",
        "\n",
        "                \"He acknowledges capitalism's role in creating class society and how bureaucratic/purposive rationality increasingly dominates individual lives. However, he argues against equating 'self-regulating systems whose imperatives override the consciousness of their members' with the 'lifeworld' - the domain of consciousness and communicative action.\",\n",
        "\n",
        "                \"Habermas's later work primarily explores the structures of the lifeworld, particularly focusing on language, communicative action, and moral consciousness.\",\n",
        "\n",
        "                \"The lifeworld is fundamentally based on an emancipatory interest, though this can be obscured by distorted uses of reason and language.\",\n",
        "\n",
        "                \"Habermas positions emancipation as the very foundation of social (and thus human) life, making his theoretical task the development of a framework that enables universal understanding of this principle.\"\n",
        "              ],\n",
        "            \"key_concepts\": [\n",
        "                \"lifeworld\",\n",
        "           \"communicative action\",\n",
        "           \"moral consciousness\",\n",
        "           \"emancipatory interest\",\n",
        "           \"language theory\",\n",
        "           \"social evolution\",\n",
        "           \"normative framework\"\n",
        "            ],\n",
        "    },\n",
        "\n",
        "    \"Lifeworld and Communicative Action\":{\n",
        "        \"description\": [\n",
        "            \"In the early 1980s, Habermas examines the concept of lifeworld by revisiting Durkheim and the phenomenological sociology of Mead and Schutz.\",\n",
        "\n",
        "           \"For Schutz, the lifeworld represented the world of everyday life and the total sphere of individual experiences - a biographically determined situation into which individuals are inevitably placed. This constitutes 'the world as taken for granted' where individuals pursue pragmatic objectives.\",\n",
        "\n",
        "           \"Habermas expands this concept, viewing the lifeworld as a horizon of consciousness encompassing both public and private spheres. He positions it as the domain of identity formation and communicative action.\",\n",
        "\n",
        "           \"Communicative action, in Habermas's framework, involves a cooperative interpretive process where participants simultaneously engage with objective, social, and subjective worlds, even when emphasizing just one component in their expressions.\",\n",
        "\n",
        "           \"Communication takes precedence in lifeworld activities because it enables individuals to validate their utterances and potentially modify lifeworld structures. These modifications are expected to influence the broader social system, potentially limiting instrumental rationality's expansion.\"\n",
        "       ],\n",
        "        \"key_concepts\": [\n",
        "            \"lifeworld as consciousness horizon\",\n",
        "           \"communicative action\",\n",
        "           \"social evolution theory\",\n",
        "           \"moral competence\",\n",
        "           \"linguistic competence\",\n",
        "           \"normative elements in interaction\"\n",
        "       ],\n",
        "    },\n",
        "\n",
        "    \"Intersubjective Recognition\": {\n",
        "        \"description\": [\n",
        "            \"Habermas emphasizes the fundamental importance of understanding basic human needs and the nature of undistorted, free communication.\",\n",
        "\n",
        "           \"He identifies an immanent feature in language itself: speakers and hearers have an a priori interest in mutual understanding, which is built into the very structure of communication.\",\n",
        "\n",
        "           \"Understanding, in Habermas's framework, requires participants to reach agreement through 'intersubjective recognition' of the validity of each other's utterances.\",\n",
        "\n",
        "           \"This communicative process necessarily leads participants to reflect on their own position within the communication, creating a self-reflexive dynamic.\",\n",
        "\n",
        "           \"Habermas argues that language's structure is fundamentally hermeneutic, requiring interpretation at all levels and enhancing individuals' self-understanding through their interactions with others.\",\n",
        "\n",
        "           \"He positions this interpretive and understanding-oriented aspect as the fundamental telos (purpose) of language itself.\",\n",
        "\n",
        "           \"Language must therefore be understood through a consensus model of rules, with its proper function being the facilitation of communication.\",\n",
        "\n",
        "           \"When communication systematically fails, Habermas identifies this as a pathological form of language use.\"\n",
        "       ],\n",
        "      \"key_concepts\": [\n",
        "          \"mutual understanding\",\n",
        "           \"intersubjective recognition\",\n",
        "           \"validity claims\",\n",
        "           \"hermeneutic structure\",\n",
        "           \"consensus model\",\n",
        "           \"communicative pathology\",\n",
        "           \"self-reflexivity\"\n",
        "       ],\n",
        "    },\n",
        "\n",
        "    \"Moral Consciousness\": {\n",
        "        \"description\": [\n",
        "            \"Habermas attempts to ground moral stages in a 'logic of development', drawing from Kohlberg, Piaget, and Mead.\",\n",
        "\n",
        "               \"He seeks to demonstrate how the moral point of view is fundamentally rooted in the structure of human life experience.\",\n",
        "\n",
        "               \"His goal is to clarify the 'universal core' of moral intuitions to counter value skepticism, though he explicitly denies making claims to moral truth.\",\n",
        "\n",
        "               \"This position creates a theoretical tension, as the refutation of value skepticism seemingly requires substantive claims about moral issues.\"\n",
        "           ],\n",
        "        \"key_concepts\": [\n",
        "            \"logic of development\",\n",
        "               \"universal core of morality\",\n",
        "               \"value skepticism\",\n",
        "               \"moral intuitions\"\n",
        "           ],\n",
        "    },\n",
        "\n",
        "    \"Discourse of Modernity\": {\n",
        "        \"description\": [\n",
        "            \"Habermas examines what he terms the 'philosophical discourse of modernity' by analyzing thinkers who resist modernity's tradition, arguing they pay 'a high price for taking leave of modernity'.\",\n",
        "\n",
        "           \"He specifically challenges Adorno, Bataille, Foucault, and Derrida for their apparent rejection of reason's fundamental rights and importance.\",\n",
        "\n",
        "           \"Habermas contends that mounting a radical critique of reason inevitably relies on reason itself, making such critiques internally contradictory.\",\n",
        "\n",
        "           \"His primary criticism is that these thinkers blur the distinction between alienation and emancipation, failing to provide clear direction toward freedom.\",\n",
        "\n",
        "           \"Habermas particularly objects to claims - echoing Adorno and Horkheimer - that modern reason and enlightenment have contributed to severe political repression.\"\n",
        "       ],\n",
        "        \"key_concepts\": [\n",
        "             \"role of reason in modernity\",\n",
        "               \"relationship between enlightenment and repression\",\n",
        "               \"possibility of critique without reason\",\n",
        "               \"nature of emancipation\"\n",
        "           ],\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "tiS1eJtoPU0y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROCESSING KNOWLEDGE BASE"
      ],
      "metadata": {
        "id": "-wQ7nczjB1Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xB8zOtF1ZRT-",
        "outputId": "2d8cb692-0c72-4b0b-ab0d-d2417b982a12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bBVotg_li9yZ",
        "outputId": "cc061bcc-d9d0-4cb9-9fef-a300664dac45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.13 (from langchain-community)\n",
            "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.13 langchain-community-0.3.13 langchain-core-0.3.28 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "t8yxP6WxjbX3",
        "outputId": "f2805676-f274-41c8-b575-1a81043acf3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch numpy huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kpmg6tzjz5o",
        "outputId": "b50b7275-51c2-408c-d1d7-792c1374efc2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WzSiE57mmaQk",
        "outputId": "5e985699-b22c-4782-9f0b-ce5e5ca955f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWg03MmuqRpu",
        "outputId": "e7d84985-d3be-4d7a-9c16-bff339fbcbd6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/298.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import S\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/answerdotai/ModernBERT-base\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "\n",
        "class HabermasKnowledgeBase:\n",
        "    def __init__(self, concepts_dict, api_key: str, api_endpoint: str):\n",
        "\n",
        "        # Configure logging\n",
        "        log_filename = f\"habermas_kb_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "        logging.basicConfig(\n",
        "            filename=log_filename,\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        self.logger.info(\"Initializing HabermasKnowledgeBase\")#logging\n",
        "\n",
        "        try:\n",
        "\n",
        "            self.api_key = api_key\n",
        "            self.api_endpoint = \"https://\" + api_endpoint\n",
        "            self.concepts = concepts_dict\n",
        "\n",
        "            self.logger.info(f\"Loaded {len(self.concepts)} concepts\")#logging\n",
        "\n",
        "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "                )\n",
        "\n",
        "            self.logger.info(\"Text splitter initialized\")#logging\n",
        "\n",
        "            self.essays = self.load_essays()\n",
        "\n",
        "            self.logger.info(f\"Loaded essays: {list(self.essays.keys())}\")#logging\n",
        "\n",
        "            self.sep_content = self.load_sep_content()\n",
        "\n",
        "            self.logger.info(\"SEP content loaded successfully\")#logging\n",
        "\n",
        "            # Define paths as class attributes\n",
        "            self.json_path = 'habermas_complete.json'\n",
        "            self.embeddings_path = 'habermas_embeddings.npy'\n",
        "            self.chroma_path = './chroma_db'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during initialization: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def load_sep_content(self):\n",
        "        sep_url = \"https://plato.stanford.edu/entries/habermas/\"\n",
        "        loader = WebBaseLoader(sep_url)\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "            sep_chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "            # Process chunks to include embeddings\n",
        "            processed_chunks = []\n",
        "            for chunk in sep_chunks:\n",
        "                embedding = self.process_text(chunk.page_content)\n",
        "                if embedding:\n",
        "                    processed_chunks.append({\n",
        "                        'content': chunk.page_content,\n",
        "                        'embedding': embedding['embeddings'],\n",
        "                        'metadata': chunk.metadata\n",
        "                    })\n",
        "            return processed_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading SEP content: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def load_essays(self):\n",
        "        # Load PDFs using LangChain\n",
        "        theory_loader = PyPDFLoader(\"Theory_of_Knowledge.pdf\")\n",
        "        ethics_loader = PyPDFLoader(\"Communicative_Ethics.pdf\")\n",
        "        social_capital_loader = PyPDFLoader(\"Communicative_Action_Social_Capital.pdf\")\n",
        "        public_sphere_loader = PyPDFLoader(\"Habermas_Public_Sphere.pdf\")\n",
        "\n",
        "        # Load and chunk documents\n",
        "        processed_essays = {\n",
        "            \"theory_of_knowledge\": self.process_document(theory_loader.load()),\n",
        "            \"communicative_ethics\": self.process_document(ethics_loader.load()),\n",
        "            \"social_capital\": self.process_document(social_capital_loader.load()),\n",
        "            \"public_sphere\": self.process_document(public_sphere_loader.load())\n",
        "        }\n",
        "        return processed_essays\n",
        "\n",
        "    def processed_essays(self, documents):\n",
        "        \"\"\"Process multiple documents by first chunking them and then generating embeddings\"\"\"\n",
        "        # First split documents into chunks\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "        # Process each chunk to get embeddings\n",
        "        processed_chunks = []\n",
        "        for chunk in chunks:\n",
        "            embedding = self.process_text(chunk.page_content)\n",
        "            if embedding:\n",
        "                processed_chunks.append({\n",
        "                    'content': chunk.page_content,\n",
        "                    'embedding': embedding['embeddings'],\n",
        "                    'metadata': chunk.metadata\n",
        "                })\n",
        "\n",
        "        return processed_chunks\n",
        "\n",
        "    def process_text(self, text: str):\n",
        "        \"\"\"Process text through API to generate embeddings\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Processing text of length: {len(text)}\")\n",
        "\n",
        "            response = requests.post(\n",
        "                self.api_key,\n",
        "                headers=headers,\n",
        "                json={\"inputs\": text}\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            self.logger.info(\"Successfully generated embeddings\")\n",
        "            return {'embeddings': response.json()}\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.logger.error(f\"API request failed: {str(e)}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Unexpected error in text processing: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_similarity(self, embedding1, embedding2):\n",
        "         # Calculate cosine similarity between two embeddings\"\"\"\n",
        "            return torch.nn.functional.cosine_similarity(\n",
        "              embedding1,\n",
        "              embedding2\n",
        "              ).item()\n",
        "\n",
        "\n",
        "    def query_knowledge(self, query: str):\n",
        "           # Query the knowledge base using ModernBERT embeddings\"\"\"\n",
        "        query_embedding = self.process_text_with_bert(query)\n",
        "\n",
        "        if not query_embedding:\n",
        "            return []\n",
        "        # Process each source and find relevant content\n",
        "        results = []\n",
        "\n",
        "        # Process concepts\n",
        "        for concept_name, concept_content in self.concepts.items():\n",
        "            try:\n",
        "                # Handle the case where concept content is a dictionary\n",
        "                concept_text = concept_content['description'][0] if isinstance(concept_content, dict) else concept_content\n",
        "                concept_embedding = self.process_text(concept_text)\n",
        "\n",
        "                if concept_embedding:\n",
        "                      similarity = torch.nn.functional.cosine_similarity(\n",
        "                          torch.tensor(query_embedding['embeddings']),\n",
        "                          torch.tensor(concept_embedding['embeddings'])\n",
        "                      ).item()\n",
        "\n",
        "                      results.append({\n",
        "                          'content': concept_content,\n",
        "                          'source': 'concepts',\n",
        "                          'name': concept_name,\n",
        "                          'similarity': similarity\n",
        "                      })\n",
        "\n",
        "            except Exception as e:\n",
        "             print(f\"Error processing concept {concept_name}: {str(e)}\")\n",
        "\n",
        "      # Process essays using the processed chunks\n",
        "        for essay_type, chunks in self.essays.items():\n",
        "          for chunk in chunks:\n",
        "              try:\n",
        "                    # The chunks are already processed with embeddings\n",
        "                  similarity = torch.nn.functional.cosine_similarity(\n",
        "                      torch.tensor(query_embedding['embeddings']),\n",
        "                      torch.tensor(chunk['embedding'])\n",
        "                  ).item()\n",
        "\n",
        "                  results.append({\n",
        "                      'content': chunk['content'],\n",
        "                      'source': essay_type,\n",
        "                      'metadata': chunk['metadata'],\n",
        "                      'similarity': similarity\n",
        "                  })\n",
        "              except Exception as e:\n",
        "                    print(f\"Error processing chunk from {essay_type}: {str(e)}\")\n",
        "\n",
        "    # Process SEP content if available\n",
        "        if self.sep_content:\n",
        "            for chunk in self.sep_content:\n",
        "                try:\n",
        "                    similarity = torch.nn.functional.cosine_similarity(\n",
        "                        torch.tensor(query_embedding['embeddings']),\n",
        "                        torch.tensor(chunk['embedding'])\n",
        "                    ).item()\n",
        "\n",
        "                    results.append({\n",
        "                        'content': chunk['content'],\n",
        "                        'source': 'SEP',\n",
        "                        'metadata': chunk['metadata'],\n",
        "                        'similarity': similarity\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing SEP chunk: {str(e)}\")\n",
        "\n",
        "        # Sort results by similarity and return top matches\n",
        "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "        return results[:3]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bCjHpYbAdxOS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVING EMBEDDINGS + DATA - locally and chroma"
      ],
      "metadata": {
        "id": "Hqxh0xhVIvg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import os\n",
        "import numpy as np\n",
        "from chromadb.config import Settings\n",
        "\n",
        "\n",
        "def save_knowledge_base(self):\n",
        "        \"\"\"Save both raw data and embeddings using multiple storage methods\"\"\"\n",
        "        # 1. Save raw data to JSON\n",
        "        self.save_raw_data()\n",
        "\n",
        "        # 2. Save embeddings to disk\n",
        "        self.save_embeddings_local()\n",
        "\n",
        "        # 3. Initialize and populate Chroma\n",
        "        self.save_to_chroma()\n",
        "\n",
        "\n",
        "def save_raw_data(self):\n",
        "    #Save raw text data to JSON\"\"\"\n",
        "    #Combine dictionary and essays into a single source\"\"\"\n",
        "    combined_data = {\n",
        "        \"structured_concepts\": habermas_analysis,  # Your existing dictionary\n",
        "        \"essays\": {\n",
        "            \"theory_of_knowledge\": theory_text,\n",
        "            \"communicative_ethics\": ethics_text,\n",
        "            \"social_capital\": social_capital_text,\n",
        "            \"public_sphere\": public_sphere_text\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open('habermas_complete.json', 'w') as f:\n",
        "        json.dump(combined_data, f, indent=2)\n",
        "\n",
        "# 1. Save embeddings_data before loading. This should be done after calculations\n",
        "\n",
        "def save_embeddings_local(self):\n",
        "        \"\"\"Save BERT embeddings to disk using numpy\"\"\"\n",
        "\n",
        "        embeddings_data = {\n",
        "            \"concepts\": {},\n",
        "            \"essays\": {},\n",
        "            \"sep\": []\n",
        "        }\n",
        "\n",
        "          # Process and save concept embeddings\n",
        "        for concept_name, content in self.concepts.items():\n",
        "            embedding = self.process_text_with_bert(content)['embeddings'].numpy()\n",
        "            embeddings_data[\"concepts\"][concept_name] = embedding\n",
        "\n",
        "        # Process and save essay embeddings\n",
        "        for essay_type, chunks in self.essays.items():\n",
        "            embeddings_data[\"essays\"][essay_type] = [\n",
        "                self.process_text_with_bert(chunk.page_content)['embeddings'].numpy()\n",
        "                for chunk in chunks\n",
        "            ]\n",
        "\n",
        "        np.save('habermas_embeddings.npy', embeddings_data)\n",
        "\n",
        "        if os.path.exists('habermas_embeddings.npy'):\n",
        "            embeddings_data = np.load('habermas_embeddings.npy', allow_pickle=True).item()\n",
        "        else:\n",
        "             print(\"Error: 'habermas_embeddings.npy' not found. Make sure to save the embeddings first.\")\n",
        "              # Handle the case where the file doesn't exist (e.g., exit the program, recalculate embeddings)\n",
        "\n",
        "\n",
        "def save_to_chroma(self):\n",
        "        \"\"\"Save embeddings to Chroma for vector search\"\"\"\n",
        "\n",
        "        # Initialize Chroma client with persistence\n",
        "        client = chromadb.PersistentClient(\n",
        "            path=\"./chroma_db\",\n",
        "            settings=Settings(\n",
        "                allow_reset=True,\n",
        "                is_persistent=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Create or get collection\n",
        "        collection = client.create_collection(\n",
        "            name=\"habermas_knowledge\",\n",
        "            metadata={\"description\": \"Habermas philosophical knowledge base\"}\n",
        "        )\n",
        "\n",
        "        # Add concepts to collection\n",
        "        for concept_name, content in self.concepts.items():\n",
        "            embedding = self.process_text_with_bert(content)['embeddings'].numpy()\n",
        "            collection.add(\n",
        "                documents=[content],\n",
        "                embeddings=[embedding],\n",
        "                metadatas=[{\"type\": \"concept\", \"name\": concept_name}],\n",
        "                ids=[f\"concept_{concept_name}\"]\n",
        "            )\n",
        "\n",
        "          # Add essay chunks to collection\n",
        "        for essay_type, chunks in self.essays.items():\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                embedding = self.process_text_with_bert(chunk.page_content)['embeddings'].numpy()\n",
        "                collection.add(\n",
        "                    documents=[chunk.page_content],\n",
        "                    embeddings=[embedding],\n",
        "                    metadatas=[{\"type\": \"essay\", \"source\": essay_type}],\n",
        "                    ids=[f\"{essay_type}_chunk_{i}\"]\n",
        "                )\n",
        "def verify_saves():\n",
        "    \"\"\"Check if all components were saved successfully\"\"\"\n",
        "    files_exist = all([\n",
        "        os.path.exists('habermas_complete.json'),\n",
        "        os.path.exists('habermas_embeddings.npy'),\n",
        "        os.path.exists('./chroma_db')\n",
        "    ])\n",
        "\n",
        "    if files_exist:\n",
        "        print(\"All data components saved successfully\")\n",
        "    else:\n",
        "        print(\"Some components are missing. Please check the save functions\")\n",
        "\n",
        "    # verification method that checks all essential components\n",
        "def verify_knowledge_base_data(json_path, embeddings_path, chroma_path):\n",
        "    \"\"\"Verify that all necessary knowledge base components are present and properly saved\"\"\"\n",
        "    verification_results = {\n",
        "        \"status\": True,\n",
        "        \"details\": []\n",
        "    }\n",
        "\n",
        "    # Verify file existence and data integrity\n",
        "    if not os.path.exists(json_path):\n",
        "        verification_results[\"status\"] = False\n",
        "        verification_results[\"details\"].append(\"JSON data file not found\")\n",
        "    else:\n",
        "        try:\n",
        "            with open(json_path, 'r') as f:\n",
        "                json_data = json.load(f)\n",
        "                verification_results[\"details\"].append(f\"JSON data loaded successfully with {len(json_data)} components\")\n",
        "        except Exception as e:\n",
        "            verification_results[\"status\"] = False\n",
        "            verification_results[\"details\"].append(f\"Error reading JSON data: {str(e)}\")\n",
        "\n",
        "    if not os.path.exists(embeddings_path):\n",
        "        verification_results[\"status\"] = False\n",
        "        verification_results[\"details\"].append(\"Embeddings file not found\")\n",
        "    else:\n",
        "        try:\n",
        "            embeddings_data = np.load(embeddings_path, allow_pickle=True).item()\n",
        "            verification_results[\"details\"].append(f\"Embeddings loaded successfully\")\n",
        "        except Exception as e:\n",
        "            verification_results[\"status\"] = False\n",
        "            verification_results[\"details\"].append(f\"Error reading embeddings: {str(e)}\")\n",
        "\n",
        "    if not os.path.exists(chroma_path):\n",
        "        verification_results[\"status\"] = False\n",
        "        verification_results[\"details\"].append(\"Chroma database not found\")\n",
        "    else:\n",
        "        try:\n",
        "            client = chromadb.PersistentClient(path=chroma_path)\n",
        "            collection = client.get_collection(\"habermas_knowledge\")\n",
        "            collection_info = collection.count()\n",
        "            verification_results[\"details\"].append(f\"Vector store contains {collection_info} entries\")\n",
        "        except Exception as e:\n",
        "            verification_results[\"status\"] = False\n",
        "            verification_results[\"details\"].append(f\"Error accessing vector store: {str(e)}\")\n",
        "\n",
        "    return verification_results\n",
        "\n",
        "    # Run verification\n",
        "verification_results = verify_knowledge_base_data(\n",
        "    json_path='habermas_complete.json',\n",
        "    embeddings_path='habermas_embeddings.npy',\n",
        "    chroma_path='./chroma_db'\n",
        ")\n",
        "\n",
        "# Print verification results with clear formatting\n",
        "print(\"Knowledge Base Verification Results:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Status: {'Success' if verification_results['status'] else 'Failed'}\")\n",
        "print(\"\\nDetailed Results:\")\n",
        "for detail in verification_results[\"details\"]:\n",
        "    print(f\"• {detail}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHY4jX-It8m",
        "outputId": "959ffe0e-97ec-4d02-b481-082e895ea947"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Base Verification Results:\n",
            "----------------------------------------\n",
            "Status: Failed\n",
            "\n",
            "Detailed Results:\n",
            "• JSON data file not found\n",
            "• Embeddings file not found\n",
            "• Chroma database not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the knowledge base\n",
        "habermas_kb = HabermasKnowledgeBase(habermas_data)\n",
        "\n",
        "# Verify initialization\n",
        "verification = habermas_kb.verify_initialization()\n",
        "\n",
        "if verification[\"status\"]:\n",
        "    print(\"Knowledge base successfully initialized\")\n",
        "    for detail in verification[\"details\"]:\n",
        "        print(f\"- {detail}\")\n",
        "else:\n",
        "    print(\"Knowledge base initialization incomplete:\")\n",
        "    for detail in verification[\"details\"]:\n",
        "        print(f\"- {detail}\")\n",
        "    raise RuntimeError(\"Knowledge base initialization failed\")"
      ],
      "metadata": {
        "id": "7PJ59dHK-qD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Use bert to generate custom embeddings. Integrate this embeddings with chroma"
      ],
      "metadata": {
        "id": "t_P33YUEEQb0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulRtzsduxNN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VECCTOR SEARCH _ CHROMA CLOUD STORAGE"
      ],
      "metadata": {
        "id": "cRz0lY8jDLlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "class BERTEmbeddings(Embeddings):\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                embedding = outputs.hidden_states[-1][:, 0, :].numpy()\n",
        "            embeddings.append(embedding[0])\n",
        "        return embeddings\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embedding = outputs.hidden_states[-1][:, 0, :].numpy()\n",
        "        return embedding[0]\n",
        "\n",
        "def initialize_vector_store(texts, model, tokenizer):\n",
        "    # Initialize custom BERT embeddings\n",
        "    bert_embeddings = BERTEmbeddings(model, tokenizer)\n",
        "\n",
        "    # Create the vector store with BERT embeddings\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=texts,\n",
        "        embedding=bert_embeddings,\n",
        "        persist_directory=\"./chroma_db\"\n",
        "    )\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "def semantic_search(query, vectorstore, k=3):\n",
        "    results = vectorstore.similarity_search_with_score(\n",
        "        query=query,\n",
        "        k=k\n",
        "    )\n",
        "    return [(doc.page_content, score) for doc, score in results]"
      ],
      "metadata": {
        "id": "pnq32BepkgCW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pV950weC6SlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rA5BaiF86SPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved numpy embeddings\n",
        "embeddings_data = np.load('habermas_embeddings.npy', allow_pickle=True).item()\n",
        "\n",
        "# Print the saved embeddings\n",
        "print(\"Concept embeddings:\")\n",
        "for concept_name, embedding in embeddings_data[\"concepts\"].items():\n",
        "    print(f\"\\nConcept: {concept_name}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "\n",
        "print(\"\\nEssay embeddings:\")\n",
        "for essay_type, embeddings in embeddings_data[\"essays\"].items():\n",
        "    print(f\"\\nEssay: {essay_type}\")\n",
        "    print(f\"Number of chunks: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "NQw4XPG3Dg82",
        "outputId": "9724d8b6-9b3b-4fed-cc61-206690e2beca"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'habermas_embeddings.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-73163197f0df>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the saved numpy embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'habermas_embeddings.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Print the saved embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concept embeddings:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'habermas_embeddings.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKBJ0wulDgYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGENT : CHARACTER + FUNCTION - between step 2-3 define personality\n"
      ],
      "metadata": {
        "id": "Qh_tb7tNCsSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Sessions for memory caching - create a better conversation for the user and a more memorable one for the agent"
      ],
      "metadata": {
        "id": "B0UG65g1SO0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "import redis\n",
        "from typing import Optional, Dict\n",
        "\n",
        "class SessionManager:\n",
        "    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n",
        "        self.redis_client = redis.Redis.from_url(redis_url)\n",
        "        self.session_expiry = timedelta(hours=24)\n",
        "\n",
        "    def create_session(self, user_id: str) -> str:\n",
        "        session_id = f\"session:{user_id}:{datetime.now().timestamp()}\"\n",
        "        session_data = {\n",
        "            'user_id': user_id,\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'dialogue_context': {},\n",
        "            'last_accessed': datetime.now().isoformat()\n",
        "        }\n",
        "        self.redis_client.setex(\n",
        "            session_id,\n",
        "            self.session_expiry,\n",
        "            json.dumps(session_data)\n",
        "        )\n",
        "        return session_id\n",
        "\n",
        "    def get_session(self, session_id: str) -> Optional[Dict]:\n",
        "        session_data = self.redis_client.get(session_id)\n",
        "        if session_data:\n",
        "            return json.loads(session_data)\n",
        "        return None\n",
        "\n",
        "    def update_session(self, session_id: str, dialogue_context: DialogueContext):\n",
        "        session_data = self.get_session(session_id)\n",
        "        if session_data:\n",
        "            session_data['dialogue_context'] = vars(dialogue_context)\n",
        "            session_data['last_accessed'] = datetime.now().isoformat()\n",
        "            self.redis_client.setex(\n",
        "                session_id,\n",
        "                self.session_expiry,\n",
        "                json.dumps(session_data)\n",
        "            )"
      ],
      "metadata": {
        "id": "VizaNkljSOZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation Cache"
      ],
      "metadata": {
        "id": "kGk0kTzMS7Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import lru_cache\n",
        "from dataclasses import asdict\n",
        "\n",
        "class ConversationCache:\n",
        "    def __init__(self, max_size: int = 1000):\n",
        "        self.cache = LRUCache(maxsize=max_size)\n",
        "\n",
        "    def store_interaction(\n",
        "        self,\n",
        "        session_id: str,\n",
        "        user_input: str,\n",
        "        response: str,\n",
        "        dialogue_context: DialogueContext\n",
        "    ):\n",
        "        self.cache[session_id] = {\n",
        "            'last_interaction': {\n",
        "                'user_input': user_input,\n",
        "                'response': response,\n",
        "                'dialogue_context': asdict(dialogue_context),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_recent_context(self, session_id: str) -> Optional[Dict]:\n",
        "        return self.cache.get(session_id)"
      ],
      "metadata": {
        "id": "ewbNc4_wS6zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the AGENT\n"
      ],
      "metadata": {
        "id": "YSPfoCEKScL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from threading import Lock\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class DialogueContext:\n",
        "    \"\"\"Maintains context for ongoing philosophical discussions\"\"\"\n",
        "    conversation_history: List[Dict[str, str]]\n",
        "    current_topic: str\n",
        "    validity_claims: Dict[str, bool]\n",
        "    understanding_level: int\n",
        "\n",
        "class HabermasAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        max_context_length: int = 2048,\n",
        "        temperature: float = 0.7\n",
        "    ):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.max_context_length = max_context_length\n",
        "        self.temperature = temperature\n",
        "        self.context_lock = Lock()\n",
        "\n",
        "          # adding in session and caching to agent state\n",
        "        self.session_manager = SessionManager()\n",
        "        self.conversation_cache = ConversationCache()\n",
        "\n",
        "        # Load philosophical principles\n",
        "        self.principles = self._load_principles()\n",
        "\n",
        "        # Initialize dialogue management\n",
        "        self.dialogue_context = DialogueContext(\n",
        "            conversation_history=[],\n",
        "            current_topic=\"\",\n",
        "            validity_claims={\"truth\": False, \"rightness\": False, \"sincerity\": False},\n",
        "            understanding_level=0\n",
        "        )\n",
        "\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _load_principles(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load Habermas's philosophical principles and frameworks\"\"\"\n",
        "        return {\n",
        "            \"communicative_action\": {\n",
        "                \"validity_claims\": [\"truth\", \"rightness\", \"sincerity\"],\n",
        "                \"prerequisites\": [\"equal_participation\", \"freedom_from_coercion\"],\n",
        "                \"goals\": [\"mutual_understanding\", \"rational_consensus\"]\n",
        "            },\n",
        "            \"discourse_ethics\": {\n",
        "                \"principles\": [\"universalization\", \"discourse\"],\n",
        "                \"conditions\": [\"inclusivity\", \"equal_rights\", \"truthfulness\"]\n",
        "            },\n",
        "            \"rationality_types\": {\n",
        "                \"communicative\": \"oriented_to_understanding\",\n",
        "                \"instrumental\": \"oriented_to_success\",\n",
        "                \"strategic\": \"oriented_to_influence\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _evaluate_validity_claims(self, utterance: str) -> Dict[str, bool]:\n",
        "        \"\"\"Evaluate if the utterance satisfies Habermas's validity claims\"\"\"\n",
        "        claims = {\n",
        "            \"truth\": self._check_truth_claim(utterance),\n",
        "            \"rightness\": self._check_normative_claim(utterance),\n",
        "            \"sincerity\": self._check_sincerity_claim(utterance)\n",
        "        }\n",
        "        return claims\n",
        "\n",
        "    def _check_truth_claim(self, utterance: str) -> bool:\n",
        "        \"\"\"Check if utterance makes verifiable truth claims\"\"\"\n",
        "        # Implement truth claim verification logic\n",
        "        return True  # Placeholder\n",
        "\n",
        "    def _check_normative_claim(self, utterance: str) -> bool:\n",
        "        \"\"\"Check if utterance aligns with normative rightness\"\"\"\n",
        "        # Implement normative verification logic\n",
        "        return True  # Placeholder\n",
        "\n",
        "    def _check_sincerity_claim(self, utterance: str) -> bool:\n",
        "        \"\"\"Check if utterance appears sincere\"\"\"\n",
        "        # Implement sincerity verification logic\n",
        "        return True  # Placeholder\n",
        "\n",
        "    def _format_prompt(self, user_input: str) -> str:\n",
        "        \"\"\"Format input according to Habermasian dialogue principles\"\"\"\n",
        "        prompt_template = f\"\"\"As a philosophical agent based on Habermas's theories, engage in dialogue while considering:\n",
        "1. Current topic: {self.dialogue_context.current_topic}\n",
        "2. Validity claims being made\n",
        "3. Goal of mutual understanding\n",
        "\n",
        "User input: {user_input}\n",
        "\n",
        "Respond in a way that:\n",
        "1. Acknowledges validity claims\n",
        "2. Promotes rational discourse\n",
        "3. Seeks genuine understanding\n",
        "4. References relevant philosophical principles\n",
        "\n",
        "Response:\"\"\"\n",
        "        return prompt_template\n",
        "\n",
        "    def _update_context(self, user_input: str, response: str):\n",
        "        \"\"\"Update dialogue context with new interaction\"\"\"\n",
        "        with self.context_lock:\n",
        "            self.dialogue_context.conversation_history.append({\n",
        "                \"user\": user_input,\n",
        "                \"agent\": response\n",
        "            })\n",
        "            # Update validity claims\n",
        "            self.dialogue_context.validity_claims = self._evaluate_validity_claims(user_input)\n",
        "            # Adjust understanding level based on interaction\n",
        "            self._update_understanding_level()\n",
        "\n",
        "    def _update_understanding_level(self):\n",
        "        \"\"\"Update the assessed level of mutual understanding\"\"\"\n",
        "        # Implement understanding assessment logic\n",
        "        pass\n",
        "\n",
        " # ADDED function to process session logic\n",
        "    async def process_query(self, query: str, session_id: Optional[str] = None):\n",
        "        if not session_id:\n",
        "            session_id = self.session_manager.create_session(str(uuid.uuid4()))\n",
        "\n",
        "        # Retrieve cached context if available\n",
        "        cached_context = self.conversation_cache.get_recent_context(session_id)\n",
        "\n",
        "        # Generate response with context awareness\n",
        "        response = await self.habermas_agent.respond(query, session_id)\n",
        "\n",
        "        # Update cache with new interaction\n",
        "        self.conversation_cache.store_interaction(\n",
        "            session_id=session_id,\n",
        "            user_input=query,\n",
        "            response=response['response'],\n",
        "            dialogue_context=self.habermas_agent.dialogue_context\n",
        "        )\n",
        "        return {\n",
        "            'session_id': session_id,\n",
        "            'response': response['response'],\n",
        "            'context': cached_context,\n",
        "            'validity_claims': response['validity_claims']\n",
        "        }\n",
        "\n",
        "    async def respond(self, user_input: str) -> str:\n",
        "        \"\"\"Generate response maintaining Habermasian dialogue principles\"\"\"\n",
        "        try:\n",
        "            # Format prompt with context\n",
        "            prompt = self._format_prompt(user_input)\n",
        "\n",
        "            # Generate response\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "            outputs = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=self.max_context_length,\n",
        "                temperature=self.temperature,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Update dialogue context\n",
        "            self._update_context(user_input, response)\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return \"I apologize, but I'm unable to provide a proper response at this moment.\"\n",
        "\n",
        "    def reflect(self) -> Dict[str, Any]:\n",
        "        \"\"\"Perform self-reflection on dialogue quality\"\"\"\n",
        "        reflection = {\n",
        "            \"validity_claims_met\": self.dialogue_context.validity_claims,\n",
        "            \"understanding_level\": self.dialogue_context.understanding_level,\n",
        "            \"topic_coherence\": self._assess_topic_coherence(),\n",
        "            \"dialogue_quality\": self._assess_dialogue_quality()\n",
        "        }\n",
        "        return reflection\n",
        "\n",
        "    def _assess_topic_coherence(self) -> float:\n",
        "        \"\"\"Assess coherence of current topic discussion\"\"\"\n",
        "        # Implement coherence assessment logic\n",
        "        return 0.0  # Placeholder\n",
        "\n",
        "    def _assess_dialogue_quality(self) -> Dict[str, float]:\n",
        "        \"\"\"Assess quality of dialogue based on Habermasian principles\"\"\"\n",
        "        # Implement dialogue quality assessment logic\n",
        "        return {\n",
        "            \"rational_discourse\": 0.0,\n",
        "            \"mutual_understanding\": 0.0,\n",
        "            \"validity_claims\": 0.0\n",
        "        }\n",
        "\n",
        "    def save_state(self, path: str):\n",
        "        \"\"\"Save agent's current state and dialogue context\"\"\"\n",
        "        state = {\n",
        "            \"dialogue_context\": vars(self.dialogue_context),\n",
        "            \"principles\": self.principles\n",
        "        }\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(state, f)\n",
        "\n",
        "    def load_state(self, path: str):\n",
        "        \"\"\"Load agent's state and dialogue context\"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            state = json.load(f)\n",
        "            self.dialogue_context = DialogueContext(**state[\"dialogue_context\"])\n",
        "            self.principles = state[\"principles\"]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    agent = HabermasAgent(\n",
        "        model_path=\"path_to_your_model\",\n",
        "        max_context_length=2048,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Example interaction\n",
        "    async def example_dialogue():\n",
        "        response = await agent.respond(\n",
        "            \"What role does rational discourse play in achieving social consensus?\"\n",
        "        )\n",
        "        print(response)\n",
        "\n",
        "        # Get agent's reflection on dialogue\n",
        "        reflection = agent.reflect()\n",
        "        print(\"Dialogue reflection:\", reflection)"
      ],
      "metadata": {
        "id": "ipAinYdvxNr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Mistral takes CLASSIFICATIONS from BERT - retrives context"
      ],
      "metadata": {
        "id": "QipVJ31JDhuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
        "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n"
      ],
      "metadata": {
        "id": "f2CWDpqFtTl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class ResponseGenerator:\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_path,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_response(self, query, context, intent):\n",
        "        prompt = self.construct_prompt(query, context, intent)\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return self.clean_response(response, prompt)\n",
        "\n",
        "    def construct_prompt(self, query, context, intent):\n",
        "        return f\"\"\"<s>[INST] You are a philosophical agent specializing in Habermas's work.\n",
        "\n",
        "Intent Classification: {intent}\n",
        "Retrieved Context: {context}\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Based on the intent and context, provide a thoughtful response that:\n",
        "1. Addresses the specific philosophical concepts relevant to the query\n",
        "2. Incorporates Habermas's perspective from the context\n",
        "3. Engages in meaningful philosophical discussion\n",
        "\n",
        "Response: [/INST]\"\"\"\n",
        "\n",
        "    def clean_response(self, response, prompt):\n",
        "        # Remove the prompt and any system instructions from the response\n",
        "        return response.replace(prompt, \"\").strip()\n",
        "\n",
        "class HabermasKnowledgeBase:\n",
        "    def __init__(self, mistral_path):\n",
        "        self.habermas_agent = HabermasAgent(\n",
        "            model_path=mistral_path,\n",
        "            max_context_length=2048,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        self.response_generator = ResponseGenerator(\n",
        "            mistral_path=mistral_path,\n",
        "            habermas_agent=self.habermas_agent\n",
        "        )\n",
        "\n",
        "        # Existing BERT initialization\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"tasksource/ModernBERT-base-nli\")\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\"tasksource/ModernBERT-base-nli\")\n",
        "\n",
        "        # Initialize Mistral response generator\n",
        "        self.response_generator = ResponseGenerator(mistral_path)\n",
        "\n",
        "        # Initialize other components\n",
        "        self.vectorstore = None\n",
        "        self.zero_shot_pipe = pipeline(\"zero-shot-classification\",\n",
        "                                     model=\"tasksource/ModernBERT-base-nli\")\n",
        "\n",
        "    async def process_query(self, query: str):\n",
        "        # 1. Classify intent using BERT\n",
        "        intent = self.zero_shot_pipe(\n",
        "            query,\n",
        "            candidate_labels=[\n",
        "                \"epistemology\",\n",
        "                \"ethics\",\n",
        "                \"social theory\",\n",
        "                \"communication theory\",\n",
        "                \"critical theory\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # 2. Retrieve relevant context\n",
        "        relevant_context = self.query_knowledge(query)\n",
        "\n",
        "        # Format context for response generation\n",
        "        formatted_context = \"\\n\".join([\n",
        "            f\"Source: {result['source']}\\n{result['content'][:500]}...\"\n",
        "            for result in relevant_context[:2]  # Use top 2 most relevant chunks\n",
        "        ])\n",
        "\n",
        "        # 3. Generate response using Mistral\n",
        "        response = self.response_generator.generate_response(\n",
        "            query=query,\n",
        "            context=formatted_context,\n",
        "            intent=intent['labels'][0]  # Use top intent\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'response': response,\n",
        "            'intent': intent['labels'][0],\n",
        "            'context_used': relevant_context,\n",
        "            'confidence': intent['scores'][0]\n",
        "        }"
      ],
      "metadata": {
        "id": "BOkEo6loOhCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fi_MOa7tOgw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}